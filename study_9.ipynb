{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 一、环境配置\n",
    "<font size=4>本教程基于PaddlePaddle 2.3.0 编写，如果你的环境不是本版本，请先参考官网安装 PaddlePaddle 2.3.0 。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.2\n"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "import paddle.nn.functional as F\r\n",
    "from paddle.nn import Conv2D,MaxPool2D,Linear,Dropout\r\n",
    "from paddle.vision.transforms import ToTensor\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "print(paddle.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 二、加载数据集\n",
    "<font size=4>本案例将会使用飞桨提供的API完成数据集的下载并为后续的训练任务准备好数据迭代器。cifar10数据集由60000张大小为32 * 32的彩色图片组成，其中有50000张图片组成了训练集，另外10000张图片组成了测试集。这些图片分为10个类别，将训练一个模型能够把图片进行正确的分类。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transform = ToTensor()\r\n",
    "cifar10_train = paddle.vision.datasets.Cifar10(mode='train',\r\n",
    "                                               transform=transform)\r\n",
    "cifar10_test = paddle.vision.datasets.Cifar10(mode='test',\r\n",
    "                                              transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 三、组建网络\n",
    "<font size=4>接下来使用飞桨定义一个使用了三个二维卷积（ Conv2D ) 且每次卷积之后使用 relu 激活函数，两个二维池化层（ MaxPool2D ），和两个线性变换层组成的分类网络，来把一个(32, 32, 3)形状的图片通过卷积神经网络映射为10个输出，这对应着10个分类的类别。</font>\n",
    "\n",
    "## 1.LetNet\n",
    "<font size=4>LeNet大体上由提取特征的三个卷积层和两个分类的全连接层组成。(图片来自网络)</font>\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/04528378411f443bb85eb70e96777c00df52c048f8d740fc98450cdc9febc548)\n",
    "<font size=4>卷积层和全连接层采用Sigmoid激活函数。三个全连接层之间插入了两个池化层来缩小特征图，以使后面的卷积层提取更大尺度的特征。池化层采用最大池化方式。原版的输出层由欧式径向基函数单元组成。此处用softmax输出单元。输出数量为分类的类别数量。LeNet原本是设计用来分类输入尺寸为32×32的手写数字图片的。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyNet(paddle.nn.Layer):\r\n",
    "    def __init__(self, num_classes=1):\r\n",
    "        super(MyNet, self).__init__()\r\n",
    "\r\n",
    "        self.conv1 = paddle.nn.Conv2D(in_channels=3, out_channels=32, kernel_size=(3, 3))\r\n",
    "        self.pool1 = paddle.nn.MaxPool2D(kernel_size=2, stride=2)\r\n",
    "\r\n",
    "        self.conv2 = paddle.nn.Conv2D(in_channels=32, out_channels=64, kernel_size=(3,3))\r\n",
    "        self.pool2 = paddle.nn.MaxPool2D(kernel_size=2, stride=2)\r\n",
    "\r\n",
    "        self.conv3 = paddle.nn.Conv2D(in_channels=64, out_channels=64, kernel_size=(3,3))\r\n",
    "\r\n",
    "        self.flatten = paddle.nn.Flatten()\r\n",
    "\r\n",
    "        self.linear1 = paddle.nn.Linear(in_features=1024, out_features=64)\r\n",
    "        self.linear2 = paddle.nn.Linear(in_features=64, out_features=num_classes)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.pool1(x)\r\n",
    "\r\n",
    "        x = self.conv2(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.pool2(x)\r\n",
    "\r\n",
    "        x = self.conv3(x)\r\n",
    "        x = F.relu(x)\r\n",
    "\r\n",
    "        x = self.flatten(x)\r\n",
    "        x = self.linear1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.linear2(x)\r\n",
    "        return x\r\n",
    "\r\n",
    "class LeNet(paddle.nn.Layer):\r\n",
    "    def __init__(self, num_classes=1):\r\n",
    "        super(LeNet, self).__init__()\r\n",
    "        self.conv1 = Conv2D(in_channels=3, out_channels=6, kernel_size=5)\r\n",
    "        self.pool1 = MaxPool2D(kernel_size=2, stride=2)\r\n",
    "        self.conv2 = Conv2D(in_channels=6, out_channels=16,  kernel_size=5)\r\n",
    "        self.pool2 = MaxPool2D(kernel_size=2, stride=2)\r\n",
    "        self.conv3 = Conv2D(in_channels=16, out_channels=120, kernel_size=1)\r\n",
    "        \r\n",
    "        self.flatten = paddle.nn.Flatten()\r\n",
    "       \r\n",
    "        self.fc1 = Linear(in_features=120*5*5,  out_features=84)\r\n",
    "        self.fc2 = Linear(in_features=84, out_features=num_classes)\r\n",
    "    \r\n",
    "    # 前向计算过程\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv1(x)\r\n",
    "        x = F.sigmoid(x)\r\n",
    "        x = self.pool1(x)\r\n",
    "        x = self.conv2(x)\r\n",
    "        x = F.sigmoid(x)\r\n",
    "        x = self.pool2(x)\r\n",
    "        x = self.conv3(x)\r\n",
    "        x = F.sigmoid(x)\r\n",
    "        x = self.flatten(x)\r\n",
    "        #x = fluid.layers.reshape(x, [-1, 120 * 50 * 50])# 将二维的卷积层输出的特征图拉伸为同等大小的1维\r\n",
    "        x = self.fc1(x)\r\n",
    "        x = F.sigmoid(x)\r\n",
    "        x = self.fc2(x)\r\n",
    "        return x\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.AlexNet\n",
    "<font size=4> 得益于硬件的发展（GPU的使用等）和各种算法的改进，在2012的 ImageNet 图像分类竞赛中，AlexeNet 以远超第二名的成绩夺冠，使得深度学习重回历史舞台，具有重大历史意义。AlexNet主要由5层卷积层和3层全连接层组成。(图片来自网络)</font>\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/400d29e800094f61a09a90ae38e3caf47fa590097e914ef79318eafe53530f68)\n",
    "\n",
    "<font size=4>采用ReLU激活函数（The Rectified Linear Unit修正线性单元）代替了LeNet中的Sigmoid激活函数。ReLU激活函数的单侧抑制特性，使得神经网络中的神经元具有了稀疏激活性，可以在 BP 的时候将梯度很好地传到较前面的网络。\n",
    "在卷积层后使用尺寸（为3）大于步长（为2）的重叠池化降低网络的过拟合。\n",
    "在全连接层之间采用DropOut层随机抛掉部分神经元以降低网络的过拟合。\n",
    "开始使用GPU加速训练、使用mini batch进行带动量的随机梯度下降、使用数据增强（GPU加速、mini batch划分现在已普遍使用。在本项目中为了比较各个模型的性能，均未使用数据增强）</font>\n",
    "\n",
    "<font size=4>由于AlexNet输入数据图像为224，要运行32像素的Cifar10数据集不现实，需要对原结构进行微调，主要在以下三点：<br></br>\n",
    "1. 横向拓宽了网络，比如原本conv1输出64个卷积结果，现在改为输出96个卷积结果。<br></br>\n",
    "2. 将conv2和conv5后面的两个最大值池化层改为均值池化层；<br></br>\n",
    "3. 将fc1和fc2的输出数据个数均改为4096。<br></br>\n",
    "修改后的网络结构如下图：</font>\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/fe9013ebd2264c169d698079ea982ef9829ada29f72f4472a0d2c383cccd87a3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AlexNet(paddle.nn.Layer):\r\n",
    "    def __init__(self, num_classes=1):\r\n",
    "        super(AlexNet, self).__init__()\r\n",
    "\r\n",
    "        self.conv1 = Conv2D(3, 64, 3, stride=1, padding=1)\r\n",
    "        self.pool1 = MaxPool2D(kernel_size=2, stride=2)\r\n",
    "        self.conv2 = Conv2D(64, 192, 3, stride=1, padding=1)\r\n",
    "        self.pool2 = MaxPool2D(kernel_size=2, stride=2)\r\n",
    "        self.conv3 = Conv2D(192, 384, 3, stride=1, padding=1)\r\n",
    "        self.conv4 = Conv2D(384, 256, 3, stride=1, padding=1)\r\n",
    "        self.conv5 = Conv2D(256, 256, 3, stride=1, padding=1)\r\n",
    "        self.pool5 = MaxPool2D(kernel_size=2, stride=2)\r\n",
    "        self.flatten = paddle.nn.Flatten()\r\n",
    "        self.fc1 = Linear(256 * 4 * 4, 2048)\r\n",
    "        self.drop_out1 = Dropout(p=0.5)\r\n",
    "        self.fc2 = Linear(2048, 1024)\r\n",
    "        self.drop_out2 = Dropout(p=0.5)\r\n",
    "        self.fc3 = Linear(1024, num_classes)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.pool1(x)\r\n",
    "        x = self.conv2(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.pool2(x)\r\n",
    "        x = self.conv3(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv4(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv5(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.pool5(x)        \r\n",
    "        x = self.flatten(x)\r\n",
    "        x = self.fc1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.drop_out1(x)\r\n",
    "        x = self.fc2(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.drop_out2(x)\r\n",
    "        x = self.fc3(x)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.VGG\n",
    "<font size=4>“VGG”代表了牛津大学的Oxford Visual Geometry Group。VGG模型采用模块化的方式将网络堆叠到了19层以增强性能。(图片来自网络)</font>\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/ed1c70900e054aa1b45f3846667610d3d76e3a7b4f3c40098990d18c195585a4)\n",
    "<font size=4>VGG网络的研究者证明了小尺寸卷积核（3x3 ）的深层网络要优于大尺寸卷积核的浅层网络，所以全部采用3×3的卷积核代替了其他的大尺寸卷积核。由于网络深度较深，因此网络权重的初始化很重要，可以通过 Xavier均匀初始化，否则可能会阻碍学习。VGG有13、16、19等多种尺度规格。训练VGG16、VGG19这样的深层网络时，可以逐层训练。先训练VGG13，然后冻结前面的层对后面的层进行微调。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VGG(paddle.nn.Layer):\r\n",
    "    def __init__(self, num_classes=1,layer=13):\r\n",
    "        super(VGG, self).__init__()\r\n",
    "\r\n",
    "        self.layer = layer\r\n",
    "\r\n",
    "        self.pool = MaxPool2D(kernel_size=2, stride=2)\r\n",
    "        self.drop_out = Dropout(p=0.5)\r\n",
    "\r\n",
    "        self.conv1 = Conv2D(3, 64, 3, padding=1)\r\n",
    "        self.conv2 = Conv2D(64, 64, 3, padding=1)\r\n",
    "\r\n",
    "        self.conv3 = Conv2D(64, 128, 3, padding=1)\r\n",
    "        self.conv4 = Conv2D(128, 128, 3, padding=1)\r\n",
    "\r\n",
    "        self.conv5 = Conv2D(128, 256, 3, padding=1)\r\n",
    "        self.conv6 = Conv2D(256, 256, 3, padding=1)\r\n",
    "        self.conv7 = Conv2D(256, 256, 3, padding=1)\r\n",
    "        self.conv8 = Conv2D(256, 256, 3, padding=1)\r\n",
    "\r\n",
    "        self.conv9 = Conv2D(256, 512, 3, padding=1)\r\n",
    "        self.conv10 = Conv2D(512, 512, 3, padding=1)\r\n",
    "        self.conv11 = Conv2D(512, 512, 3, padding=1)\r\n",
    "        self.conv12 = Conv2D(512, 512, 3, padding=1)\r\n",
    "\r\n",
    "        self.conv13 = Conv2D(512, 512, 3, padding=1)\r\n",
    "        self.conv14 = Conv2D(512, 512, 3, padding=1)\r\n",
    "        self.conv15 = Conv2D(512, 512, 3, padding=1)\r\n",
    "        self.conv16 = Conv2D(512, 512, 3, padding=1)\r\n",
    "\r\n",
    "        self.flatten = paddle.nn.Flatten()\r\n",
    "\r\n",
    "        self.fc1 = Linear(in_features=512, out_features=4096)\r\n",
    "        self.fc2 = Linear(in_features=4096, out_features=4096)\r\n",
    "        self.fc3 = Linear(in_features=4096, out_features=num_classes)\r\n",
    "\r\n",
    "    # 网络的前向计算过程\r\n",
    "    def forward(self, x):\r\n",
    "        x = F.relu(self.conv1(x))\r\n",
    "        x = F.relu(self.conv2(x))\r\n",
    "        x = self.pool(x)\r\n",
    "\r\n",
    "        x = F.relu(self.conv3(x))\r\n",
    "        # x = F.relu(x)\r\n",
    "        x = F.relu(self.conv4(x))\r\n",
    "        x = self.pool(x)\r\n",
    "\r\n",
    "        x = F.relu(self.conv5(x))\r\n",
    "        x = F.relu(self.conv6(x))\r\n",
    "        if self.layer >= 16:\r\n",
    "            x = F.relu(self.conv7(x))\r\n",
    "        if self.layer >= 19:\r\n",
    "            x = F.relu(self.conv8(x))\r\n",
    "        x = self.pool(x)\r\n",
    "\r\n",
    "        x = F.relu(self.conv9(x))\r\n",
    "        x = F.relu(self.conv10(x))\r\n",
    "        if self.layer >= 16:\r\n",
    "            x = F.relu(self.conv11(x))\r\n",
    "        if self.layer >= 19:\r\n",
    "            x = F.relu(self.conv12(x))\r\n",
    "        x = self.pool(x)\r\n",
    "\r\n",
    "        x = F.relu(self.conv13(x))\r\n",
    "        x = F.relu(self.conv14(x))\r\n",
    "        if self.layer >= 16:\r\n",
    "            x = F.relu(self.conv15(x))\r\n",
    "        if self.layer >= 19:\r\n",
    "            x = F.relu(self.conv16(x))\r\n",
    "        x = self.pool(x)\r\n",
    "        x = self.flatten(x)\r\n",
    "        #x = fluid.layers.reshape(x, [-1, 512 * 7 * 7])\r\n",
    "        x = F.relu(self.fc1(x))\r\n",
    "        x = self.drop_out(x)\r\n",
    "        x = F.relu(self.fc2(x))\r\n",
    "        x = self.drop_out(x)\r\n",
    "        x = self.fc3(x)\r\n",
    "        return x\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 四、模型训练&预测\n",
    "<font size=4>接下来，用一个循环来进行模型的训练，将会:使用 paddle.optimizer.Adam 优化器来进行优化。使用 F.cross_entropy 来计算损失值。使用 paddle.io.DataLoader 来加载数据并组建batch。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training ... \n",
      "epoch: 0, batch_id: 0, loss is: [2.3600495]\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 10\r\n",
    "batch_size = 100\r\n",
    "learning_rate = 0.001\r\n",
    "val_acc_history = []\r\n",
    "val_loss_history = []\r\n",
    "paddle.device.set_device('gpu:0')\r\n",
    "def train(model):\r\n",
    "    print('start training ... ')\r\n",
    "    # turn into training mode\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    opt = paddle.optimizer.Adam(learning_rate=learning_rate,\r\n",
    "                                parameters=model.parameters())\r\n",
    "\r\n",
    "    train_loader = paddle.io.DataLoader(cifar10_train,\r\n",
    "                                        shuffle=True,\r\n",
    "                                        batch_size=batch_size)\r\n",
    "\r\n",
    "    valid_loader = paddle.io.DataLoader(cifar10_test, batch_size=batch_size)\r\n",
    "    \r\n",
    "    for epoch in range(epoch_num):\r\n",
    "        for batch_id, data in enumerate(train_loader()):\r\n",
    "            x_data = data[0]\r\n",
    "            y_data = paddle.to_tensor(data[1])\r\n",
    "            y_data = paddle.unsqueeze(y_data, 1)\r\n",
    "\r\n",
    "            logits = model(x_data)\r\n",
    "            loss = F.cross_entropy(logits, y_data)\r\n",
    "\r\n",
    "            if batch_id % 1000 == 0:\r\n",
    "                print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, loss.numpy()))\r\n",
    "            loss.backward()\r\n",
    "            opt.step()\r\n",
    "            opt.clear_grad()\r\n",
    "        # evaluate model after one epoch\r\n",
    "        model.eval()\r\n",
    "        accuracies = []\r\n",
    "        losses = []\r\n",
    "        for batch_id, data in enumerate(valid_loader()):\r\n",
    "            x_data = data[0]\r\n",
    "            y_data = paddle.to_tensor(data[1])\r\n",
    "            y_data = paddle.unsqueeze(y_data, 1)\r\n",
    "\r\n",
    "            logits = model(x_data)\r\n",
    "            loss = F.cross_entropy(logits, y_data)\r\n",
    "            acc = paddle.metric.accuracy(logits, y_data)\r\n",
    "            accuracies.append(acc.numpy())\r\n",
    "            losses.append(loss.numpy())\r\n",
    "\r\n",
    "        avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\r\n",
    "        print(\"[validation] accuracy/loss: {}/{}\".format(avg_acc, avg_loss))\r\n",
    "        val_acc_history.append(avg_acc)\r\n",
    "        val_loss_history.append(avg_loss)\r\n",
    "        model.train()\r\n",
    "\r\n",
    "model = VGG(num_classes=10)\r\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(val_acc_history, label = 'validation accuracy')\r\n",
    "\r\n",
    "plt.xlabel('Epoch')\r\n",
    "plt.ylabel('Accuracy')\r\n",
    "plt.ylim([0, 1])\r\n",
    "plt.legend(loc='lower right')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
